{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:31:23.347074Z",
     "iopub.status.busy": "2025-09-13T18:31:23.346274Z",
     "iopub.status.idle": "2025-09-13T18:31:36.343560Z",
     "shell.execute_reply": "2025-09-13T18:31:36.342804Z",
     "shell.execute_reply.started": "2025-09-13T18:31:23.347038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Step 1: Import Libraries & Configure GPU\n",
    "# ================================\n",
    "\n",
    "# Core libraries for data handling and visualization\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras for building and training the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications, optimizers, losses\n",
    "\n",
    "# Scikit-learn for evaluation metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------\n",
    "# Enable Mixed Precision\n",
    "# -------------------------\n",
    "# Mixed precision speeds up training on GPUs with Tensor cores and reduces memory usage\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"✅ Mixed precision training enabled.\")\n",
    "\n",
    "# -------------------------\n",
    "# GPU Memory Configuration\n",
    "# -------------------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ GPU memory growth enabled for {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"⚠️ GPU configuration error: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Set Random Seed for Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "print(f\"✅ Random seed set to {SEED} for reproducibility.\")\n",
    "\n",
    "print(\"All libraries imported and environment configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:31:36.345269Z",
     "iopub.status.busy": "2025-09-13T18:31:36.344660Z",
     "iopub.status.idle": "2025-09-13T18:31:36.351076Z",
     "shell.execute_reply": "2025-09-13T18:31:36.350248Z",
     "shell.execute_reply.started": "2025-09-13T18:31:36.345248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 2: Define Configuration and Paths\n",
    "# ========================================\n",
    "\n",
    "# --- Define your new broad age groups here ---\n",
    "# Using a '01_', '02_' prefix ensures they are sorted correctly by name\n",
    "AGE_GROUPS = {\n",
    "    '01_Child': (1, 12),\n",
    "    '02_Teenager': (13, 19),\n",
    "    '03_YoungAdult': (20, 39),\n",
    "    '04_MiddleAgedAdult': (40, 59),\n",
    "    '05_Senior': (60, 120)  # High upper limit to include all older ages\n",
    "}\n",
    "NUM_CLASSES = len(AGE_GROUPS)\n",
    "\n",
    "# --- Original source paths (read-only) ---\n",
    "SOURCE_TRAIN_DIR = '/kaggle/input/age-prediction-dataset/content/organized_dataset'\n",
    "SOURCE_VALID_DIR = '/kaggle/input/age-prediction-test-datasets/test/'\n",
    "\n",
    "# --- New writable paths for your reorganized dataset ---\n",
    "WORKING_DIR = '/kaggle/working/'\n",
    "GROUPED_TRAIN_DIR = os.path.join(WORKING_DIR, 'grouped_train/')\n",
    "GROUPED_VALID_DIR = os.path.join(WORKING_DIR, 'grouped_valid/')\n",
    "\n",
    "# --- Model and training constants ---\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "INITIAL_EPOCHS = 25  # Increased epochs as EarlyStopping will handle it\n",
    "FINE_TUNE_EPOCHS = 25\n",
    "\n",
    "print(f\"Project configured for {NUM_CLASSES} age groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:06:37.435637Z",
     "iopub.status.busy": "2025-09-14T06:06:37.435317Z",
     "iopub.status.idle": "2025-09-14T06:06:37.505432Z",
     "shell.execute_reply": "2025-09-14T06:06:37.504455Z",
     "shell.execute_reply.started": "2025-09-14T06:06:37.435615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SOURCE_TRAIN_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3404625062.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Run the reorganization for both training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mreorganize_original_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE_TRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGROUPED_TRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAGE_GROUPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mreorganize_original_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE_VALID_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGROUPED_VALID_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAGE_GROUPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SOURCE_TRAIN_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# ## Step 3: Reorganize Original Datasets into Broad Categories\n",
    "# =============================================================\n",
    "\n",
    "def reorganize_original_data(source_dir, destination_dir, age_groups, is_train_set=True):\n",
    "    \"\"\"\n",
    "    Copies images directly from the original source folders into new destination folders.\n",
    "    \"\"\"\n",
    "    print(f\"Reorganizing data from {source_dir}...\")\n",
    "    for group_name in age_groups.keys():\n",
    "        os.makedirs(os.path.join(destination_dir, group_name), exist_ok=True)\n",
    "        \n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"ERROR: Source directory not found at {source_dir}\"); return\n",
    "\n",
    "    for folder_name in sorted(os.listdir(source_dir)):\n",
    "        source_folder_path = os.path.join(source_dir, folder_name)\n",
    "        if not os.path.isdir(source_folder_path): continue\n",
    "            \n",
    "        try:\n",
    "            # Extract age number from folder name\n",
    "            age = int(folder_name.split('_')[-1]) if is_train_set else int(folder_name)\n",
    "\n",
    "            # Find which group this age belongs to and copy files\n",
    "            for group_name, (min_age, max_age) in age_groups.items():\n",
    "                if min_age <= age <= max_age:\n",
    "                    destination_group_path = os.path.join(destination_dir, group_name)\n",
    "                    for image_file in os.listdir(source_folder_path):\n",
    "                        source_path = os.path.join(source_folder_path, image_file)\n",
    "                        if os.path.isfile(source_path):\n",
    "                            shutil.copy(source_path, destination_group_path)\n",
    "                    break\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "            \n",
    "    print(f\"Reorganization complete. New dataset is at: {destination_dir}\\n\")\n",
    "\n",
    "# Run the reorganization for both training and validation sets\n",
    "reorganize_original_data(SOURCE_TRAIN_DIR, GROUPED_TRAIN_DIR, AGE_GROUPS, is_train_set=True)\n",
    "reorganize_original_data(SOURCE_VALID_DIR, GROUPED_VALID_DIR, AGE_GROUPS, is_train_set=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:37:34.592502Z",
     "iopub.status.busy": "2025-09-13T18:37:34.592293Z",
     "iopub.status.idle": "2025-09-13T18:37:40.602599Z",
     "shell.execute_reply": "2025-09-13T18:37:40.601848Z",
     "shell.execute_reply.started": "2025-09-13T18:37:34.592484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## Step 4: Create High-Performance tf.data Pipelines (Corrected)\n",
    "# ================================================================\n",
    "\n",
    "def create_dataset(directory, augment=False):\n",
    "    # Load the dataset from the directory\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMG_SIZE,\n",
    "        shuffle=True if augment else False\n",
    "    )\n",
    "    \n",
    "    # Define the data augmentation pipeline as separate layers\n",
    "    data_augmentation = models.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.1),\n",
    "        # --- THIS LINE HAS BEEN CORRECTED ---\n",
    "        layers.RandAugment(value_range=(0, 255), num_ops=3) # Renamed 'augmentations_per_image' to 'num_ops'\n",
    "    ], name='data_augmentation')\n",
    "\n",
    "    # Rescale pixel values from [0, 255] to [0, 1]\n",
    "    rescale = layers.Rescaling(1./255)\n",
    "    \n",
    "    # Apply transformations\n",
    "    dataset = dataset.map(lambda x, y: (rescale(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Use buffered prefetching to load data in the background\n",
    "    if not augment:\n",
    "        dataset = dataset.cache()\n",
    "        \n",
    "    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = create_dataset(GROUPED_TRAIN_DIR, augment=True)\n",
    "valid_ds = create_dataset(GROUPED_VALID_DIR)\n",
    "\n",
    "print(\"High-performance tf.data pipelines created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:37:40.603417Z",
     "iopub.status.busy": "2025-09-13T18:37:40.603236Z",
     "iopub.status.idle": "2025-09-13T18:37:42.983233Z",
     "shell.execute_reply": "2025-09-13T18:37:42.982503Z",
     "shell.execute_reply.started": "2025-09-13T18:37:40.603402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 5: Build the High-Performance Model\n",
    "# ============================================\n",
    "\n",
    "def build_model(num_classes):\n",
    "    base_model = applications.EfficientNetV2B0(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x) # BN before Dense can sometimes be more stable\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x) # Output layer in float32 for stability\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = build_model(NUM_CLASSES)\n",
    "print(\"Model built with EfficientNetV2B0 base.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:37:42.984212Z",
     "iopub.status.busy": "2025-09-13T18:37:42.983935Z",
     "iopub.status.idle": "2025-09-13T21:38:16.294336Z",
     "shell.execute_reply": "2025-09-13T21:38:16.293571Z",
     "shell.execute_reply.started": "2025-09-13T18:37:42.984192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## Step 6: Train the Model Head (Corrected)\n",
    "# ============================================\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # <-- ADD THIS LINE\n",
    "\n",
    "# CosineDecay is a modern learning rate schedule that often leads to better results\n",
    "lr_schedule = optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3, \n",
    "    decay_steps=len(train_ds) * INITIAL_EPOCHS\n",
    ")\n",
    "\n",
    "optimizer = optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "print(\"\\nStarting initial training of the model head... 🚀\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=INITIAL_EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:38:16.298006Z",
     "iopub.status.busy": "2025-09-13T21:38:16.297800Z",
     "iopub.status.idle": "2025-09-13T21:38:16.586973Z",
     "shell.execute_reply": "2025-09-13T21:38:16.586072Z",
     "shell.execute_reply.started": "2025-09-13T21:38:16.297990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## Step 7: Fine-Tune the Model\n",
    "# ==============================\n",
    "\n",
    "# Unfreeze the base model to allow its weights to be updated\n",
    "base_model.trainable = True\n",
    "\n",
    "# Re-compile the model with a very low learning rate for fine-tuning\n",
    "# We use a new cosine decay schedule starting from a much smaller learning rate\n",
    "finetune_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-5, \n",
    "    decay_steps=len(train_ds) * FINE_TUNE_EPOCHS\n",
    ")\n",
    "\n",
    "# Use the same AdamW optimizer but with the new, lower learning rate\n",
    "optimizer_finetune = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=finetune_lr_schedule, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer_finetune,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel re-compiled for fine-tuning.\")\n",
    "model.summary()\n",
    "\n",
    "# Continue training from where the last phase left off\n",
    "total_epochs = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
    "initial_epoch_for_fine_tune = history.epoch[-1] + 1 if history.epoch else 0\n",
    "\n",
    "print(\"\\nStarting fine-tuning... ✨\")\n",
    "\n",
    "history_fine_tune = model.fit(\n",
    "    train_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=initial_epoch_for_fine_tune,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[checkpoint, early_stopping] # Re-use the same callbacks\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-13T21:38:16.587605Z",
     "iopub.status.idle": "2025-09-13T21:38:16.587941Z",
     "shell.execute_reply": "2025-09-13T21:38:16.587790Z",
     "shell.execute_reply.started": "2025-09-13T21:38:16.587774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## Step 8: Visualize Results and Evaluate\n",
    "# ==========================================\n",
    "\n",
    "# Combine training histories from both phases for a complete plot\n",
    "# Use .get() to safely access keys that might not exist if a training phase was skipped\n",
    "acc = history.history.get('accuracy', []) + history_fine_tune.history.get('accuracy', [])\n",
    "val_acc = history.history.get('val_accuracy', []) + history_fine_tune.history.get('val_accuracy', [])\n",
    "loss = history.history.get('loss', []) + history_fine_tune.history.get('loss', [])\n",
    "val_loss = history.history.get('val_loss', []) + history_fine_tune.history.get('val_loss', [])\n",
    "\n",
    "# Check if training actually happened before trying to plot\n",
    "if acc:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    # Add a vertical line to show where fine-tuning started\n",
    "    if len(history.epoch) > 0 and len(history.epoch) < INITIAL_EPOCHS:\n",
    "        plt.axvline(len(history.epoch) - 1, color='gray', linestyle='--', label='Start Fine-Tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    if len(history.epoch) > 0 and len(history.epoch) < INITIAL_EPOCHS:\n",
    "        plt.axvline(len(history.epoch) - 1, color='gray', linestyle='--', label='Start Fine-Tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "# Load the best weights that were saved by ModelCheckpoint\n",
    "print(\"\\n--- Loading best model and performing final evaluation ---\")\n",
    "model.load_weights('best_model.keras')\n",
    "final_loss, final_accuracy = model.evaluate(valid_ds, verbose=0)\n",
    "print(f\"Final Validation Loss: {final_loss:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_accuracy*100:.2f}%\")\n",
    "\n",
    "# --- Generate Classification Report and Confusion Matrix ---\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "# Get the ground truth labels from the validation dataset\n",
    "y_true = np.concatenate([y for x, y in valid_ds], axis=0)\n",
    "y_true_indices = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Make predictions on the validation dataset\n",
    "y_pred_probs = model.predict(valid_ds)\n",
    "y_pred_indices = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get the class names from the generator\n",
    "class_labels = list(valid_ds.class_names)\n",
    "\n",
    "# Print the detailed classification report\n",
    "print(classification_report(y_true_indices, y_pred_indices, target_names=class_labels))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_true_indices, y_pred_indices)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-trained Models and Creating Inference Pipeline\n",
    "Let's load our pre-trained generalist and specialist models and create a pipeline for age prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-Step Age Prediction Pipeline\n",
    "\n",
    "Our age prediction system uses a sophisticated three-step approach:\n",
    "\n",
    "1. **Feature Extraction** (VGG-Face Model)\n",
    "   - Uses `vgg_face_weights.h5`\n",
    "   - Extracts rich facial features from images\n",
    "\n",
    "2. **Generalist Model**\n",
    "   - Uses `generalist_model.h5` and `label_encoder.pkl`\n",
    "   - Predicts broad age group categories\n",
    "\n",
    "3. **Specialist Model**\n",
    "   - Uses `specialist_model.h5`\n",
    "   - Makes precise age predictions within the predicted group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the pre-trained models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m generalist_model = \u001b[43mtf\u001b[49m.keras.models.load_model(\u001b[33m'\u001b[39m\u001b[33mbest_generalist_model.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m specialist_model = tf.keras.models.load_model(\u001b[33m'\u001b[39m\u001b[33mbest_specialist_model.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_image\u001b[39m(image_path, target_size=(\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)):\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Import pickle for loading the label encoder\n",
    "import pickle\n",
    "\n",
    "# Load all required models\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# 1. Load VGG-Face model for feature extraction\n",
    "vgg_model = tf.keras.models.load_model('vgg_face_weights.h5')\n",
    "print(\"✅ VGG-Face feature extractor loaded\")\n",
    "\n",
    "# 2. Load Generalist model and label encoder\n",
    "generalist_model = tf.keras.models.load_model('generalist_model.h5')\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "print(\"✅ Generalist model and label encoder loaded\")\n",
    "\n",
    "# 3. Load Specialist model\n",
    "specialist_model = tf.keras.models.load_model('specialist_model.h5')\n",
    "print(\"✅ Specialist model loaded\")\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess an image for model inference\n",
    "    \"\"\"\n",
    "    # Read and resize the image\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
    "    # Convert to array and add batch dimension\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    # Rescale the image\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array\n",
    "\n",
    "def extract_features(image_array):\n",
    "    \"\"\"\n",
    "    Extract facial features using VGG-Face model\n",
    "    \"\"\"\n",
    "    features = vgg_model.predict(image_array, verbose=0)\n",
    "    return features\n",
    "\n",
    "def predict_age(image_path):\n",
    "    \"\"\"\n",
    "    Three-step age prediction process:\n",
    "    1. Extract facial features using VGG-Face\n",
    "    2. Predict age group using Generalist model\n",
    "    3. Predict precise age using Specialist model\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess image and extract features\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    features = extract_features(processed_image)\n",
    "    \n",
    "    # Step 2: Get age range prediction from generalist model\n",
    "    range_prediction = generalist_model.predict(features, verbose=0)\n",
    "    predicted_range_idx = np.argmax(range_prediction[0])\n",
    "    predicted_range = label_encoder.inverse_transform([predicted_range_idx])[0]\n",
    "    \n",
    "    # Get the age range bounds\n",
    "    min_age, max_age = AGE_GROUPS[predicted_range]\n",
    "    \n",
    "    # Step 3: Get specific age prediction from specialist model\n",
    "    # Create combined input for specialist model [features, age_group_one_hot]\n",
    "    age_group_one_hot = tf.keras.utils.to_categorical(predicted_range_idx, num_classes=len(AGE_GROUPS))\n",
    "    specialist_input = [features, np.expand_dims(age_group_one_hot, 0)]\n",
    "    \n",
    "    age_prediction = specialist_model.predict(specialist_input, verbose=0)\n",
    "    \n",
    "    # Scale the specialist model's prediction to the predicted range\n",
    "    predicted_age = min_age + (max_age - min_age) * age_prediction[0][0]\n",
    "    predicted_age = round(float(predicted_age))\n",
    "    \n",
    "    return {\n",
    "        'age_group': predicted_range,\n",
    "        'predicted_age': predicted_age,\n",
    "        'confidence': float(range_prediction[0][predicted_range_idx]),\n",
    "        'features': features  # Include features in case needed for further analysis\n",
    "    }\n",
    "\n",
    "print(\"✅ Age prediction pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Pipeline\n",
    "Let's test our model pipeline with a sample image to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline with a sample image\n",
    "# Replace 'sample_image.jpg' with the path to your test image\n",
    "test_image_path = 'sample_image.jpg'  # Update this path\n",
    "\n",
    "try:\n",
    "    result = predict_age(test_image_path)\n",
    "    \n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"------------------\")\n",
    "    print(f\"1. Age Group: {result['age_group']}\")\n",
    "    print(f\"2. Predicted Age: {result['predicted_age']} years\")\n",
    "    print(f\"3. Confidence: {result['confidence']*100:.2f}%\")\n",
    "    \n",
    "    # Display the test image with predictions\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    img = tf.keras.preprocessing.image.load_img(test_image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Age Prediction Results\\n\" + \n",
    "              f\"Predicted Age: {result['predicted_age']} years\\n\" +\n",
    "              f\"Group: {result['age_group']}\\n\" +\n",
    "              f\"Confidence: {result['confidence']*100:.1f}%\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Test image file not found. Please update the test_image_path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing the pipeline: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8260111,
     "sourceId": 13044609,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8260133,
     "sourceId": 13044639,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
